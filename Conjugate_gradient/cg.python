import numpy as np

def conjugate_gradient(A, b, tol=1e-6, max_iter=1000):
    n = len(b)
    x = np.zeros(n)
    r = b - np.dot(A, x)
    p = r.copy()
    rsold = np.dot(r, r)
    
    for i in range(max_iter):
        Ap = np.dot(A, p)
        alpha = rsold / np.dot(p, Ap)
        x += alpha * p
        r -= alpha * Ap
        rsnew = np.dot(r, r)
        if np.sqrt(rsnew) < tol:
            print(f"Converged in {i+1} iterations")
            break
        beta = rsnew / rsold
        p = r + beta * p
        rsold = rsnew
    else:
        print("Did not converge within max iterations")
    return x

# Define matrix A and vector b
A = np.array([[4, -1, 0],
              [-1, 4, -1],
              [0, -1, 4]], dtype=float)
b = np.array([1.0, 2.0, 3.0])

# Solve using Conjugate Gradient
x = conjugate_gradient(A, b, tol=1e-6)

# Output the solution
print("Solution x:", x)

# Validate the solution
residual = np.dot(A, x) - b
print("Residual vector:", residual)
print("Residual norm:", np.linalg.norm(residual))

# Exact solution for comparison
x_exact = np.linalg.solve(A, b)
print("Exact solution from np.linalg.solve:", x_exact)
